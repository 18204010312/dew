=== 测试、生产环境安装配置

[IMPORTANT]
====
本文会给出使用代理与不使用代理的安装、配置方式，但强烈推荐使用代理方式，详见 <<proxies>> 。
====

[NOTE]
====
本文各服务使用 ``x.dew.ms`` 做为访问的域名，可根据实际情况替换。
====

==== 服务规划

TIP: 常规的项目研发多会分 ``开发(dev)、测试(test)、预发(pre-prod)/仿真(uat)、生产(prod)`` 等多个环境。

服务整体上三类：

. 公共支撑服务，如 Gitlab 、 Harbor 等，这些服务要求所有环境共用、互通
. 每个环境独立部署的支撑服务， 如 RabbitMQ、PostgreSql、Redis、dnsmasq、Minio 等，出于数据、资源隔离的要求这些服务各个环境分别部署
. 每个环境的Docker与Kubernetes集群，原则上各环境使用独立的集群，如果共用集群时需要使用``namespace``加以区分

.推荐的服务列表
|===
|分类 | 域名/主机名 | 服务 | 备注

| 公共支撑服务 | domain:gitlab.dew.ms | Gitlab | Gitlab及其CI/CD服务
| 公共支撑服务 | domain:harbor.dew.ms | Harbor | Docker私有库服务
| 公共支撑服务 | domain:maven.dew.ms | Maven | Maven私有库服务
| 环境相关的支撑服务 | / | Dnsmasq | 轻量级DNS解析服务
| 环境相关的支撑服务 | domain:minio.dew.ms | Minio | 分布式对象存储服务
| 容器集群 | hostname:k8s-X | Docker | Docker容器服务，部署到所有Kubernetes所在的节点
| 容器集群 | hostname:k8s-X | <CNI> | Kubernetes CNI服务，部署到所有Kubernetes所在的节点，本文使用 Flannel
| 容器集群 | hostname:k8s-masterX | Kubernetes Master | Kubernetes Master服务，可做HA
| 容器集群 | hostname:k8s-masterX | Helm tiller | Helm服务，部署到Kubernetes Master所在节点
| 容器集群 | hostname:k8s-nodeX | Kubernetes Node | Kubernetes Node服务，至少3个节点
|===

[IMPORTANT]
====
* 容器集群各节点主机名与IP的映射配置到 /etc/hosts 中
* kubernetes Node 应至少分两个组: ``group=app`` 用于运行应用， ``group=devops`` 用于运行运维管理工具。
====

TIP: 各支撑服务（中间件）的安装见  <<middleware>> ，下文介绍容器服务的安装配置。

==== 基础配置

*以 Centos7 为例，各节点做好ssh免密互访、关闭防火墙、关闭swap、禁用SELINUX*

[source,bash]
----
# 关闭防火墙
systemctl stop firewalld.service
systemctl disable firewalld.service
# 关闭swap
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
# 禁用SELINUX
sed -i s/^SELINUX=.*$/SELINUX=disabled/ /etc/selinux/config
# 创建key
ssh-keygen -t rsa
----

[source,bash]
----
# 每个节点执行完上述命令后再执行ssh复制，每个节点都要执行N次（N为节点数-1）
ssh-copy-id -i ~/.ssh/id_rsa.pub root@k8s-X
----

==== Docker

TIP: https://kubernetes.io/docs/setup/cri/#docker

[source,bash]
----
yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum update -y && yum install -y docker-ce-18.06.2.ce

mkdir /etc/docker

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF


# 添加代理（可选）
mkdir -p /etc/systemd/system/docker.service.d/
cat >>/etc/systemd/system/docker.service.d/http-proxy.conf <<EOF
[Service]
Environment="HTTP_PROXY=http://<代理host>:<代理端口>" "HTTPS_PROXY=http://<代理host>:<代理端口>" "NO_PROXY=localhost,127.0.0.1,dew.ms"
EOF

systemctl daemon-reload
systemctl restart docker
systemctl enable docker
----

==== kubernetes

TIP: https://kubernetes.io/docs/setup/independent/install-kubeadm/

[source,bash]
.安装
----
# 使用阿里云镜像加速下载
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl enable --now kubelet
----

TIP: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

[source,bash]
.Master配置
----
# 安装Git，后续会用到
yum install -y git

# 初始化Kubernetes，二选一，使用代理方式
kubeadm init \
    --pod-network-cidr=10.244.0.0/16

# 初始化Kubernetes，二选一，不使用代理方式，通过image-repository 及 --kubernetes-version 避免被墙
kubeadm init \
    --image-repository registry.aliyuncs.com/google_containers \
    --kubernetes-version v1.14.1 \
    --pod-network-cidr=10.244.0.0/16

# 记录上述操作输出中的kubeadm join
# e.g.
# kubeadm join 10.200.10.10:6443 --token i3i7qw.2gst6kayu1e8ezlg --discovery-token-ca-cert-hash sha256:cabc90823a8e0bcf6e3bf719abc569a47c186f6cfd0e156ed5a3cd5a8d85fab0

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 查看集群状态
kubectl get cs

# 安装flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml

# 都为Running后表示完成
kubectl get pods --all-namespaces

# 创建命名空间，方便后文使用
kubectl create ns devops
----

[NOTE]
.Master做为Node
====
默认情况下 master 不会做为 node 节点，可通过此命令强制启用（不推荐）

``kubectl taint nodes --all node-role.kubernetes.io/master-``
====

TIP: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

[source,bash]
.Node配置
----
# 执行上一步输出的 kubeadm join ...

# 完成后在master上执行情况如下（以1.14.1版本为例）
kubectl get no
-
k8s-master1   Ready    master   22m     v1.14.1
k8s-node1     Ready    <none>   11m     v1.14.1
k8s-node2     Ready    <none>   8m54s   v1.14.1
k8s-node3     Ready    <none>   8m51s   v1.14.1
k8s-node4     Ready    <none>   8m49s   v1.14.1
-
----

[source,bash]
.Master HA配置
----
# @see https://kubernetes.io/docs/setup/independent/high-availability/
----

[source,bash]
.Node功能划分（打label）
----
kubectl label nodes k8s-nodeX k8s-nodeX ...  group=app
kubectl label nodes k8s-nodeX k8s-nodeX ...  group=devops
----

[source,bash]
.添加外部DNS服务，如dnsmasq
----
# 编辑Kubernetes的DNS，加上dew.ms的代理
kubectl -n kube-system edit cm coredns
-
data:
  Corefile: |
    ...
    dew.ms:53 {
        errors
        cache 30
        proxy . x.x.x.x
    }
-
----

==== Helm

TIP: https://docs.helm.sh/using_helm/#installing-helm

[source,bash]
----
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

# 初始化服务，二选一，使用代理方式
helm init --service-account tiller

# 初始化服务，二选一，不使用代理方式，需要指定镜像，注意tiller版本和helm版本对应
helm init --service-account tiller -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1
# 或者初始化之后更换镜像
kubectl set image deployment/tiller-deploy tiller=registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 -n kube-system

# 查看helm版本
helm version

kubectl get pod -n kube-system -l app=helm
----

==== Nginx Ingress Controller

[source,bash]
----
# 使用如下方式将80 443暴露出来
helm install stable/nginx-ingress --name dew-nginx --namespace ingress-nginx \
    --set controller.kind=DaemonSet \
    --set controller.hostNetwork=true \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true \
    --set nodeSelector.group=devops
----

==== dashboard

[source,bash]
----
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque
EOF

# 安装，不使用代理方式需要加上 --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64
helm install stable/kubernetes-dashboard --name dew-dashboard --namespace kube-system \
    --set rbac.clusterAdminRole=true \
    --set serviceAccount.create=true \
    --set ingress.enabled=true \
    --set-string ingress.annotations."nginx\.ingress\.kubernetes\.io/backend-protocol"="HTTPS" \
    --set ingress.hosts={dashboard.dew.ms} \
    --set ingress.tls[0].hosts={dashboard.dew.ms},ingress.tls[0].secretName=kubernetes-dashboard-certs \
    --set nodeSelector.group=devops

# 获取Token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep dew-dashboard-kubernetes-dashboard | awk '{print $1}')

# 添加域名到客户机hosts并访问 https://dashboard.dew.ms

----

==== elasticsearch

TIP: https://github.com/elastic/helm-charts/blob/master/elasticsearch 注意仔细查看各参数设值的说明。

[source,bash]
----
# 创建PV
app=dew-elasticsearch-client
size=200Gi
# 请根据replicas的个数来决定下面PV的创建个数
for i in {0..1}; do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${app}
  name: ${app}-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/elasticsearch/${app}-${i}
    server: nfs.dew.ms
EOF
done

# 注意在NFS服务器上创建对应文件夹
for i in {0..1}; do
mkdir -p /data/nfs/elasticsearch/${app}-${i}
done

# TIP：如果pod没有启动成功，报错和路径权限问题有关，可尝试给PV的存储路径添加权限,如：
chmod 775 /data/nfs/elasticsearch/dew-elasticsearch-client-0
chmod 775 /data/nfs/elasticsearch/dew-elasticsearch-client-1


# 使用helm安装
helm repo add elastic https://helm.elastic.co

helm install --name dew-elasticsearch elastic/elasticsearch --namespace devops \
    --set imageTag=6.6.1 \
    --set clusterName=dew-elasticsearch \
    --set nodeGroup=client \
    --set masterService=dew-elasticsearch-client \
    --set replicas=2 \
    --set minimumMasterNodes=2 \
    --set volumeClaimTemplate.storageClassName="" \
    --set volumeClaimTemplate.resources.requests.storage=200Gi \
    --set fsGroup=0 \
    --set clusterHealthCheckParams="" \
    --set ingress.enabled=true \
    --set ingress.hosts={es.dew.ms}

    # pod调度相关配置,请根据需要进行设值
    --set nodeSelector."tag"="devops" \
    --set tolerations[0].key="key" \
    --set tolerations[0].operator="Equal" \
    --set tolerations[0].value="value" \
    --set tolerations[0].effect="NoSchedule" \
    --set nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="tag" \
    --set nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=devops \
    --set antiAffinity="hard" \  #该值可为soft
    --set antiAffinityTopologyKey="kubernetes.io/hostname" \

    # 若使用xpack security,请加上以下参数
    --set-string extraEnvs[0]."name"="xpack\.security\.enabled" \
    --set-string extraEnvs[0]."value"="true" \
    --set-string extraEnvs[1]."name"="xpack\.security\.authc\.accept_default_password" \
    --set-string extraEnvs[1]."value"="true" \
    --set-string extraEnvs[2]."name"="ELASTIC_USERNAME" \
    --set-string extraEnvs[2]."value"="elastic" \
    --set-string extraEnvs[3]."name"="ELASTIC_PASSWORD" \
    --set-string extraEnvs[3]."value"="123456"

    xpack安装相关文档说明：https://github.com/elastic/helm-charts/blob/master/elasticsearch/README.md#security

    * 开启xpack security的简单例子：
      . 进入容器内部
        kubectl exec -it dew-elasticsearch-client-0 -n devops /bin/sh
      . 激活30天试用license
        curl -H "Content-Type:application/json" -XPOST  http://localhost:9200/_xpack/license/start_trial?acknowledge=true
      . 修改密码：
        bin/elasticsearch-setup-passwords interactive
      . 测试：
        curl -u elastic -XGET 'localhost:9200/_cat/health?v&pretty'

----

TIP: 其他elasticsearch的helm chart : https://github.com/helm/charts/tree/master/stable/elasticsearch

==== fluentd

TIP: https://github.com/kiwigrid/helm-charts/tree/master/charts/fluentd-elasticsearch +
     https://kiwigrid.github.io/

[source,bash]
----
helm repo add kiwigrid https://kiwigrid.github.io

helm install kiwigrid/fluentd-elasticsearch --name dew-fluentd-es --namespace devops \
    --set elasticsearch.host=dew-elasticsearch-client \
    --set elasticsearch.logstash_prefix=logstash \
    # 若 ES 启用 xpack 的 security，加上以下参数
    --set elasticsearch.user=elastic \
    --set elasticsearch.password=123456
    # Prometheus 相关设置(需先安装prometheus-operator)
    --set service.type=ClusterIP \
    --set service.ports[0].name="monitor-agent" \
    --set service.ports[0].port=24231 \
    --set prometheusRule.enabled=true \
    --set prometheusRule.prometheusNamespace=devops \
    --set prometheusRule.labels.app=prometheus-operator \
    --set prometheusRule.labels.release=dew-prometheus-operator \
    --set serviceMonitor.enabled=true \
    --set serviceMonitor.labels.release=dew-prometheus-operator
    # 不使用代理要加上
    --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/fluentd-elasticsearch \
    --set image.tag=v2.4.0
    # pod调度相关配置,请根据实际需要进行设值
    --set nodeSelector."tag"="devops" \
    --set tolerations[0].key="key" \
    --set tolerations[0].operator="Equal" \
    --set tolerations[0].value="value" \
    --set tolerations[0].effect="NoSchedule"
----


==== kibana

TIP: https://github.com/helm/charts/tree/master/stable/kibana

[source,bash]
----

使用PVC
app=("kibana")
size=10Gi

for i in ${app[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: ${size}
  selector:
    matchLabels:
      app: ${i}
---
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/${i}
    server: nfs.dew.ms
EOF
done
# 注意在NFS服务器上加上文件路径,并创建需要的目录

helm install --name dew-kibana stable/kibana --namespace devops \
    --set image.tag="6.6.1" \
    --set env."ELASTICSEARCH_URL"="http://dew-elasticsearch-client:9200" \
    --set service.internalPort=5601 \
    --set ingress.enabled=true,ingress.hosts={kibana.dew.ms} \
    --set-string ingress.annotations."kubernetes\.io/ingress\.class"=nginx \
    --set-string ingress.annotations."kubernetes\.io/tls-acme"="true" \
    --set ingress.tls[0].hosts={kibana.dew.ms},ingress.tls[0].secretName=kibana-certs \
    --set dashboardImport.enabled=true \
    --set dashboardImport.dashboards."k8s"="https://raw.githubusercontent.com/monotek/kibana-dashboards/master/k8s-fluentd-elasticsearch.json" \
    --set serviceAccount.create=true,serviceAccountName=kibana \
    --set plugins.enabled=true \
    --set persistentVolumeClaim.enabled=true \
    --set persistentVolumeClaim.existingClaim=true \
    --set securityContext.enabled=true \
    --set securityContext.allowPrivilegeEscalation=true \
    --set securityContext.runAsUser=0 \
    --set securityContext.fsGroup=0

    # pod调度相关配置，请根据实际情况设值
    --set nodeSelector."tag"="devops" \
    --set tolerations[0].key="key" \
    --set tolerations[0].operator="Equal" \
    --set tolerations[0].value="value" \
    --set tolerations[0].effect="NoSchedule" \
    --set affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \

    # xpack security相关参数：
    --set image.repository=docker.elastic.co/kibana/kibana \
    --set env."XPACK_SECURITY_ENABLED"="true" \
    --set env."ELASTICSEARCH_USERNAME"="kibana" \
    --set env."ELASTICSEARCH_PASSWORD"="dew123456" \
    --set dashboardImport.xpackauth.enabled=true \
    --set dashboardImport.xpackauth.username=kibana\
    --set dashboardImport.xpackauth.password=dew123456

----

==== jaeger

TIP: https://github.com/jaegertracing/jaeger-operator

[source,bash]
----
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/crds/jaegertracing_v1_jaeger_crd.yaml
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role_binding.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/operator.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -

# 使用elasticsearch作为jaeger的数据源
    # 若ES启用Xpack Security，则需要创建secret
    ELASTICSEARCH_USERNAME=elastic
    ELASTICSEARCH_PASSWORD=123456
    cat <<EOF | kubectl -n devops apply -f -
    apiVersion: v1
    kind: Secret
    metadata:
      name: jaeger-es-secrets
    type: Opaque
    data:
      ES_USERNAME: `echo -n $ELASTICSEARCH_USERNAME | base64`
      ES_PASSWORD: `echo -n $ELASTICSEARCH_PASSWORD | base64`
    EOF

    # 创建Jaeger实例
    cat <<EOF | kubectl apply -n devops -f -
    apiVersion: jaegertracing.io/v1
    kind: Jaeger
    metadata:
      name: jaeger
    spec:
      strategy: production
      storage:
        type: elasticsearch
        options:
          es:
            server-urls: http://dew-elasticsearch-client:9200
        secretName: jaeger-es-secrets  # 若ES启用Xpack Security，需要设置此项及创建secret
    EOF

    TIP: Jaeger实例可在不同namespace下创建使用，使用中请注意namespace的问题。
    使用sidecar的方式部署项目：https://github.com/jaegertracing/jaeger-operator#auto-injection-of-jaeger-agent-sidecars
    使用daemonset的方式部署项目：https://github.com/jaegertracing/jaeger-operator#agent-as-daemonset

# 修改Ingress
cat <<EOF | kubectl -n devops apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  name: jaeger-query
spec:
  rules:
    - host: jaeger.dew.ms
      http:
        paths:
          - backend:
              serviceName: jaeger-query
              servicePort: 16686
            path: /
EOF

----

.pod的调度
目前jaeger-operator暂不支持直接设置，请关注该项目的更新情况。
可以自行给需要调度的pod的deployment添加限制条件。可参考： <<podAssignment>>

.Jaeger demo
[source,bash]
----
cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    inject-jaeger-agent: "true"
    sidecar.jaegertracing.io/inject: "true"
  name: jaeger-demo
spec:
  template:
    metadata:
      labels:
        app: jaeger-demo
        version: v1
    spec:
      containers:
      - name: jaeger-demo
        image: jaegertracing/example-hotrod:1.10
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    inject-jaeger-agent: "true"
    sidecar.jaegertracing.io/inject: "true"
  name: jaeger-demo
  labels:
    app: jaeger-demo
spec:
  ports:
   - name: jaeger-demo
     port: 8080
     targetPort: 8080
  selector:
   app: jaeger-demo
EOF
----


==== prometheus-operator 和 grafana
TIP: https://github.com/helm/charts/tree/master/stable/prometheus-operator

  prometheus-operator结构：
    |--- prometheus-operator
    |--- prometheus
    |--- alertmanager
    |--- node-exporter
    |--- kube-state-metrics
    |--- service monitors to scrape internal kubernetes components
    |     |---kube-apiserver
    |     |---kube-scheduler
    |     |---kube-controller-manager
    |     |---etcd
    |     |---kube-dns/coredns
    |
    |--- grafana

===== prometheus-operator 使用PV

[source,bash]
----
# 创建PV,注意label的对应

app=prometheus-operator
components=("alertmanager" "prometheus")
size=100Gi

for i in ${components[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    component: ${i}
  name: dew-${app}-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/${app}/${i}
    server: nfs.dew.ms
EOF
done

# 在NFS服务器上创建相同路径
for i in ${components[@]};do
mkdir -p /data/nfs/${app}/${i}
done

----

===== 创建grafana的PVC和PV
[source,bash]
----
app=("grafana")
size=50Gi

for i in ${app[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: ${size}
  selector:
    matchLabels:
      app: ${i}
---
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/prometheus-operator/${i}
    server: nfs.dew.ms
EOF
done

# 注意在NFS服务器上创建相同路径
mkdir -p /data/nfs/prometheus-operator/grafana
----


===== 使用helm 安装

注意安装前先更新chart仓库 +
`helm repo update`

[source,yaml]
----
# 若需要对etcd进行监控，则需要先创建secret
kubectl -n devops create secret generic dew-prometheus-operator-etcd  --from-file=/etc/kubernetes/pki/etcd/ca.crt  --from-file=/etc/kubernetes/pki/etcd/peer.crt  --from-file=/etc/kubernetes/pki/etcd/peer.key

helm install stable/prometheus-operator --name dew-prometheus-operator --namespace devops \
    --set kubelet.serviceMonitor.https=true \
    --set prometheus.ingress.enabled=true \
    --set prometheus.ingress.hosts={prometheus.dew.ms} \
    --set alertmanager.ingress.enabled=true \
    --set alertmanager.ingress.hosts={prometheus.alertmanager.ms} \
    --set prometheusOperator.securityContext.runAsNonRoot=false \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.selector.matchLabels."component"="alertmanager" \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.selector.matchLabels."component"="prometheus"
    # 对etcd监测相关参数
    --set prometheus.prometheusSpec.secrets[0]=dew-prometheus-operator-etcd \
    --set kubeEtcd.serviceMonitor.scheme=https \
    --set kubeEtcd.serviceMonitor.insecureSkipVerify=true \
    --set kubeEtcd.serviceMonitor.caFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/ca.crt" \
    --set kubeEtcd.serviceMonitor.certFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/peer.crt" \
    --set kubeEtcd.serviceMonitor.keyFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/peer.key"
    # 直接使用prometheus-operator的grafana，添加以下设置
    --set grafana.enabled=true \
    --set grafana.adminPassword=Dew123456 \
    --set grafana.defaultDashboardsEnabled=true \
    --set grafana.ingress.enabled=true \
    --set grafana.ingress.hosts={grafana.dew.ms} \
    --set grafana.ingress.tls[0].host={grafana.dew.ms},ingress.tls[0].secretName=dew-grafana \
    --set grafana.sidecar.dashboards.enabled=true \
    --set grafana.sidecar.dashboards.searchNamespace="devops"\
    --set grafana.sidecar.dashboards.label=grafana_dashboard \
    --set grafana.sidecar.datasources.enabled=true \
    --set grafana.sidecar.datasources.searchNamespace="devops" \
    --set grafana.sidecar.datasources.label=grafana_datasource \
    --set grafana.'grafana\.ini'.smtp.enabled="true" \
    --set grafana.'grafana\.ini'.smtp.host="smtp.163.com:25" \
    --set grafana.'grafana\.ini'.smtp.user=XXXXX@163.com \
    --set grafana.'grafana\.ini'.smtp.password=XXXXX \
    --set grafana.'grafana\.ini'.smtp.from_address="XXXXX@163.com" \
    --set grafana.'grafana\.ini'.smtp.skip_verify=true \
    --set grafana.persistence.enabled=true \
    --set grafana.persistence.existingClaim=dew-grafana
    # 可选设置
    --set grafana.'grafana\.ini'.server.root_url="https://grafana.dew.ms"

    # 如不使用代理,更换以下镜像仓库
    --set kube-state-metrics.image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-state-metrics

TIP: grafana默认用户名：admin,
查看密码：
kubectl get secret --namespace devops dew-prometheus-operator-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
grafana重置密码：进入grafana的容器内部后执行
grafana-cli admin reset-admin-password passwordvalue

INFO: 若有pod启动失败,报文件权限拒绝相关问题，很可能和PV的文件目录的权限有关，检查下权限是否一致，设置对应的securityContext进行排查。
例：
kubectl edit statefulset prometheus-dew-prometheus-operator-prometheus -n devops
设置securityContext为以下内容
      securityContext:
        fsGroup: 0
        runAsNonRoot: false
        runAsUser: 0

INFO: 若通过UI查看prometheus的target中，kube-scheduler、kube-controller处于down状态，是因为它们只能在宿主机上通过127.0.0.1访问，可使用以下操作：
    . 如果使用kubeadm启动的集群，初始化时的config.yml里可以加入如下参数
        controllerManagerExtraArgs:
          address: 0.0.0.0
        schedulerExtraArgs:
          address: 0.0.0.0
    . 已经启动后的使用下面命令更改就会滚动更新
        sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yaml
        sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml
      或者全部替换：
        sed -ri '/--address/s#=.+#=0.0.0.0#' /etc/kubernetes/manifests/kube-*
    . 参考文章：
      http://www.servicemesher.com/blog/prometheus-operator-manual/
      https://github.com/coreos/prometheus-operator/blob/master/Documentation/troubleshooting.md

# 监控APP
  1.首先需要将项目instrument
    参考文章：https://prometheus.io/docs/instrumenting/clientlibs/
  2.部署项目及创建进行监控的ServiceMonitor。
    注意ServiceMonitor的labels要含有Prometheus-operator创建的Prometheus的serviceMonitorSelector的label。
    详细文章：https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md#related-resources
----

[[prometheus-pod-assignment]]
===== pod的调度
pod调度helm安装相关参数，以下配置仅供参考
[source,yaml]
----
    --set alertmanager.alertmanagerSpec.nodeSelector."tag"="devops" \
    --set alertmanager.alertmanagerSpec.tolerations[0].key="key" \
    --set alertmanager.alertmanagerSpec.tolerations[0].operator="Equal" \
    --set alertmanager.alertmanagerSpec.tolerations[0].value="value" \
    --set alertmanager.alertmanagerSpec.tolerations[0].effect="NoSchedule" \
    # podAntiAffinity的值可以 hard 或 soft
    --set alertmanager.alertmanagerSpec.podAntiAffinity="hard" \
    --set alertmanager.alertmanagerSpec.podAntiAffinityTopologyKey="kubernetes\.io/hostname" \

    --set prometheusOperator.nodeSelector."tag"="devops" \
    --set prometheusOperator.tolerations[0].key="key" \
    --set prometheusOperator.tolerations[0].operator="Equal" \
    --set prometheusOperator.tolerations[0].value="value" \
    --set prometheusOperator.tolerations[0].effect="NoSchedule" \
    --set prometheusOperator.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set prometheusOperator.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set prometheusOperator.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \

    --set prometheus.prometheusSpec.nodeSelector."tag"="devops" \
    --set prometheus.prometheusSpec.tolerations[0].key="key" \
    --set prometheus.prometheusSpec.tolerations[0].operator="Equal" \
    --set prometheus.prometheusSpec.tolerations[0].value="value" \
    --set prometheus.prometheusSpec.tolerations[0].effect="NoSchedule" \
    # podAntiAffinity的值可以 hard 或 soft
    --set prometheus.prometheusSpec.podAntiAffinity=hard \
    --set prometheus.prometheusSpec.podAntiAffinityTopologyKey="kubernetes\.io/hostname" \

    --set kube-state-metrics.nodeSelector."tag"="devops" \
    --set kube-state-metrics.tolerations[0].key="key" \
    --set kube-state-metrics.tolerations[0].operator="Equal" \
    --set kube-state-metrics.tolerations[0].value="value" \
    --set kube-state-metrics.tolerations[0].effect="NoSchedule"

    --set nodeExporter.nodeSelector."tag"="devops" \
    --set nodeExporter.tolerations[0].key="key" \
    --set nodeExporter.tolerations[0].operator="Equal" \
    --set nodeExporter.tolerations[0].value="value" \
    --set nodeExporter.tolerations[0].effect="NoSchedule" \
    --set nodeExporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set nodeExporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set nodeExporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \

    --set grafana.nodeSelector."tag"="devops" \
    --set grafana.tolerations[0].key="key" \
    --set grafana.tolerations[0].operator="Equal" \
    --set grafana.tolerations[0].value="value" \
    --set grafana.tolerations[0].effect="NoSchedule"
    --set grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \
----

===== 卸载
[source,yaml]
----
helm del --purge dew-prometheus-operator

kubectl delete crd prometheuses.monitoring.coreos.com prometheusrules.monitoring.coreos.com servicemonitors.monitoring.coreos.com alertmanagers.monitoring.coreos.com

kubectl delete pvc -n devops prometheus-dew-prometheus-operator-prometheus-db-prometheus-dew-prometheus-operator-prometheus-0 alertmanager-dew-prometheus-operator-alertmanager-db-alertmanager-dew-prometheus-operator-alertmanager-0

最后注意删除自己创建的PV、PVC
----
