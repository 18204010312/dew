=== 测试、生产环境安装配置

[IMPORTANT]
====
本文会给出使用代理与不使用代理的安装、配置方式，但强烈推荐使用代理方式，详见 <<proxies>> 。
====

*以 Centos7 为例，做好ssh免密互访、关闭防火墙、关闭swap、禁用SELINUX*

[source,bash]
----
systemctl stop firewalld.service
systemctl disable firewalld.service
swapoff -someField
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
sed -i s/^SELINUX=.*$/SELINUX=disabled/ /etc/selinux/config
----

.服务列表
|===
|主机名 |IP(示例) |服务 | 备注

|devops(1-3) | 10.200.10.1-3 | Gitlab | Gitlab及其CI/CD独立一台服务器部署，实际环境应该做HA
|middleware(1-6) | 10.200.10.4-9 | Docker、RabbitMQ、PostgreSql、Redis、dnsmasq、Minio、harbor | 各类中间件，实际环境应该做HA
|k8s-master(1-3) | 10.200.10.10-12 | Docker、kubernetes master、Helm | Kubernetes Master节点，实际环境应该做HA
|k8s-node(1-N) | 10.200.10.13-N | Docker、kubernetes node、nginx Ingress Controller、prometheus node exporter、fluentd | 应用运行容器组 node label = app
|…… | …… | Docker、kubernetes node | 应用运行容器组 node label = app
|k8s-node(N+1-M) | 10.200.10.N+1-M | Docker、kubernetes node、nginx Ingress Controller、prometheus node exporter、fluentd、dashboard、elasticsearch、kibana、jaeger、grafana、prometheus | 集成运维容器组 node label = devops
|…… | …… | Docker、kubernetes node | 集成运维容器组 node label = devops
|===

NOTE: kubernetes Node 分两个 label, ``label=app`` 用于运行应用， ``label-devops`` 用于运行运维管理工具。

[source,bash]
.各节点Host(示例)
----
# dnsmasq服务节点

cat >>/etc/hosts <<EOF
10.200.10.1 devops1 gitlab.dew.ms
10.200.10.2 devops2 gitlab.dew.ms
...
10.200.10.4 middleware1 rabbitmq.dew.ms redis.dew.ms postgre.dew.ms minio.dew.ms
10.200.10.5 middleware2 rabbitmq.dew.ms redis.dew.ms postgre.dew.ms minio.dew.ms
...
10.200.10.10 k8s-master1
10.200.10.11 k8s-master2
...
10.200.10.13 k8s-node1
10.200.10.14 k8s-node2
...
10.200.10.101 k8s-node101 harbor.dew.ms notary.dew.ms dashboard.dew.ms es.dew.ms jaeger.dew.ms kibana.dew.ms prometheus.dew.ms grafana.dew.ms
10.200.10.102 k8s-node102 harbor.dew.ms notary.dew.ms dashboard.dew.ms es.dew.ms jaeger.dew.ms kibana.dew.ms prometheus.dew.ms grafana.dew.ms
...
EOF

# 除dnsmasq外的各节点

cat >>/etc/hosts <<EOF
10.200.10.1 devops1
10.200.10.2 devops2
...
10.200.10.4 middleware1
10.200.10.5 middleware2
...
10.200.10.10 k8s-master1
10.200.10.11 k8s-master2
...
10.200.10.13 k8s-node1
10.200.10.14 k8s-node2
...
10.200.10.101 k8s-node101
10.200.10.102 k8s-node102
...
EOF
----

==== docker

TIP: https://kubernetes.io/docs/setup/cri/#docker

[source,bash]
----
yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

yum update -y && yum install -y docker-ce-18.06.2.ce

mkdir /etc/docker

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# 添加代理
cat >>/etc/systemd/system/docker.service.d/http-proxy.conf <<EOF
[Service]
Environment="HTTP_PROXY=http://<代理host>:<代理端口>" "HTTPS_PROXY=http://<代理host>:<代理端口>" "NO_PROXY=localhost,127.0.0.1,dew.ms"
EOF

systemctl daemon-reload
systemctl restart docker
systemctl enable docker.service
----

==== kubernetes

TIP: https://kubernetes.io/docs/setup/independent/install-kubeadm/

[source,bash]
.安装
----
# 使用阿里云镜像
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
systemctl enable --now kubelet
----

TIP: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

[source,bash]
.Master配置
----
# 安装Git，后续会用到
yum install -y git

# 初始化Kubernetes，二选一，使用代理方式
kubeadm init \
    --kubernetes-version v1.14.0 \
    --pod-network-cidr=10.244.0.0/16

# 初始化Kubernetes，二选一，不使用代理方式，通过image-repository 及 --kubernetes-version 避免被墙
kubeadm init \
    --image-repository registry.aliyuncs.com/google_containers \
    --kubernetes-version v1.14.0 \
    --pod-network-cidr=10.244.0.0/16

# 记录上述操作输出中的kubeadm join
# e.g.
kubeadm join 10.200.10.10:6443 --token i3i7qw.2gst6kayu1e8ezlg --discovery-token-ca-cert-hash sha256:cabc90823a8e0bcf6e3bf719abc569a47c186f6cfd0e156ed5a3cd5a8d85fab0

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 查看集群状态
kubectl get cs

# 安装flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml

# 都为Running后表示完成
kubectl get pods --all-namespaces

# 创建命名空间，方便后文使用
kubectl create ns devops
----

[NOTE]
.Master做为Node
====
默认情况下 master 不会做为 node 节点，可通过此命令强制启用（不推荐） +
``kubectl taint nodes --all node-role.kubernetes.io/master-``
====

TIP: https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

[source,bash]
.Node配置
----
# 执行上一步输出的 kubeadm join ...

# 完成后在master上执行情况如下（以1.14.0版本为例）
kubectl get no
NAME        STATUS     ROLES    AGE   VERSION
test1.k8s   Ready   master   11m   v1.14.0
test2.k8s   Ready   <none>   70s   v1.14.0
test3.k8s   Ready   <none>   52s   v1.14.0
test4.k8s   Ready   <none>   43s   v1.14.0
test5.k8s   Ready   <none>   34s   v1.14.0
----

[source,bash]
.Master HA配置
----
TBD
----

==== helm

TIP: https://docs.helm.sh/using_helm/#installing-helm

[source,bash]
----

curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF

helm init --service-account tiller
# 不使用代理方式，需要指定镜像，注意tiller版本和helm版本对应
helm init --service-account tiller -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1
# 或者初始化之后更换镜像
kubectl set image deployment/tiller-deploy tiller=registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 -n kube-system

# 查看helm版本
helm version

kubectl get pod -n kube-system -l app=helm
----

==== dns

[source,bash]
----
# 在middlewarer的某个节点上执行安装
yum install -y dnsmasq
systemctl enable dnsmasq
systemctl start dnsmasq

# 编辑各节点，加上dnsmasq节点的IP（示例为10.200.10.4）
vi /etc/resolv.conf
-
nameserver 10.200.10.4
-

# TIP: 以上设置在节点重启后可能被重置，更好的做法见：
# https://unix.stackexchange.com/questions/163831/nameservers-erased-after-systemctl-restart-network-service

# 编辑Kubernetes的DNS，加上dew.ms的代理（示例为10.200.10.4）
kubectl -n kube-system edit cm coredns
-
data:
  Corefile: |
    ...
    dew.ms:53 {
        errors
        cache 30
        proxy . 10.200.10.4
    }
-
----

==== minio

[source,bash]
----
# 在middlewarer的某个节点上执行安装
wget https://dl.minio.io/server/minio/release/linux-amd64/minio
chmod +x minio
nohup ./minio server /mnt/data &

# cat nohup.out，输出内容示例如下：
# AccessKey: F1HR1NUAPVQVX3UPV73P
# SecretKey: 0+vzU8IK+UjJTepBEiAt9x7QO5k+vYRW2KpISWVs
#
# Browser Access:
#    http://10.200.10.5:9000  http://172.17.0.1:9000 ...

# 访问 http://minio.dew.ms:9000
# 修改访问AccessKey和SecretKey， e.g. dew / Dew123456
# 创建名为 app-cache 的bucket用于缓存gitlab ci runner(或其它CICD服务）的构建缓存
----

[source,bash]
.MinIO HA配置
----
TBD https://docs.min.io/docs/distributed-minio-quickstart-guide.html
----

[source,bash]
.MinIO 多用户配置
----
TBD https://docs.min.io/docs/minio-multi-user-quickstart-guide.html
----

==== postgreSql

IMPORTANT: 生产环境需要HA部署。

[source,bash]
----
wget https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-redhat96-9.6-3.noarch.rpm

rpm -Uvh pgdg-redhat96-9.6-3.noarch.rpm
yum install -y postgresql96-server

/usr/pgsql-9.6/bin/postgresql96-setup initdb

vi /var/lib/pgsql/9.6/data/postgresql.conf
-
listen_addresses='*'
-

vi /var/lib/pgsql/9.6/data/pg_hba.conf
-
host  all  all 0.0.0.0/0 md5
-

systemctl enable postgresql-9.6.service
systemctl start postgresql-9.6.service

su - postgres
psql -U postgres
-
ALTER USER postgres WITH PASSWORD 'Dew!123456';
-
----

==== redis

IMPORTANT: 生产环境需要HA部署。

[source,bash]
----
yum install -y epel-release
yum -y install redis
vi /etc/redis.conf
-
# 注释
# bind 127.0.0.1
# 开启密码
requirepass Dew!123456
-
systemctl start redis
----

==== gitlab

TIP: https://docs.gitlab.com/omnibus/README.html#installation-and-configuration-using-omnibus-package

[source,bash]
----
curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash
yum install -y gitlab-ce

# 按需修改，可修改说明见: https://docs.gitlab.com/omnibus/settings/
vi /etc/gitlab/gitlab.rb
-
external_url 'http://gitlab.dew.ms'
...
-
gitlab-ctl reconfigure

# 浏览器访问并修改root密码

# 安装 gitlab runner，Helm方式
helm repo add gitlab https://charts.gitlab.io
helm fetch --untar gitlab/gitlab-runner
cd gitlab-runner

# 添加Cache secret
kubectl create secret generic minio-access -n devops \
    --from-literal=accesskey="dew" \
    --from-literal=secretkey="Dew123456"


# 自定义Maven settings.xml(可选)
cat <<EOF | kubectl apply -f -
kind: ConfigMap
apiVersion: v1
metadata:
  name: dew-maven-settings
  namespace: devops
data:
  settings.xml: |-
     <?xml version="1.0" encoding="UTF-8"?>
     <settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
               xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">
         <servers>
             <!--示例，添加一个私有库认证-->
             <server>
                 <id>trc-repo</id>
                 <username>dew</username>
                 <password>Dew123456</password>
             </server>
         </servers>
     </settings>
EOF



# 注意添加的位置在 “# Start the runner” 前
vi templates/configmap.yaml
-
    cat >>/home/gitlab-runner/.gitlab-runner/config.toml <<EOF
            [[runners.kubernetes.volumes.config_map]]
              name = "dew-maven-settings"
              mount_path = "/opt/maven"
    EOF
    # Start the runner
-

# 为每个项目的每个集群环境添加Runner
helm install --name <projectName> --namespace devops \
    --set gitlabUrl=http://gitlab.dew.ms/ \
    --set runnerRegistrationToken=<...> \ # 需要从gitlab页面上获取
    --set rbac.create=true \
    --set rbacWideAccess=true \
    --set runners.tags=<profile> \ # 当前集群环境名称
    --set runners.image=dewms/devops:latest \
    --set runners.cache.cacheType=s3 \
    --set runners.cache.cacheShared=true \
    --set runners.cache.s3ServerAddress=minio.dew.ms:9000 \
    --set runners.cache.s3BucketName=app-cache \
    --set runners.cache.s3CacheInsecure=true \
    --set runners.cache.secretName=minio-access \
    --set runners.env.MAVEN_OPTS="-Dmaven.repo.local=.m2 -Dorg.apache.maven.user-settings=/opt/maven/settings.xml" \
    --set runners.env.dew_devops_profile=test \
    --set runners.env.dew_devops_quiet=true \
    --set runners.env.dew_devops_docker_host=<...> \
    --set runners.env.dew_devops_docker_registry_url=https://harbor.dew.ms/v2 \
    --set runners.env.dew_devops_docker_registry_username=<...> \
    --set runners.env.dew_devops_docker_registry_password=<...> \
    --set runners.env.dew_devops_kube_config=<...> \
   .

----
.gitlab pod的调度
可通过helm安装时，设置以下各具体值来指定pod要调度的node。 +
具体可参考： <<prometheus-pod-assignment,prometheus pod的调度>>
[source,bash]
----
   --set affinity: {}
   --set nodeSelector: {}
   --set tolerations: []
----

[source,bash]
.Gitlab HA配置
----
TBD
----

==== harbor（TBD）

TBD Harbor独立部署以确保各环境使用同一套

TIP: https://github.com/goharbor/harbor-helm

[source,bash]
----
git clone https://github.com/goharbor/harbor-helm
cd harbor-helm
git checkout 1.0.0

# 创建Postgres数据库
-
CREATE DATABASE  registry;
CREATE DATABASE  clair;
CREATE DATABASE  notary_server;
CREATE DATABASE  notary_signer;
-

# 初始用户名/密码 admin/Harbor12345

# 访问 https://harbor.dew.ms

# 获取证书
kubectl -n devops get secrets/dew-harbor-harbor-ingress -o jsonpath="{.data.ca\.crt}" | base64 --decode

# 以下操作在每台服务上执行

mkdir -p /etc/docker/certs.d/harbor.dew.ms
cat <<EOF > /etc/docker/certs.d/harbor.dew.ms/ca.crt
<上一步获取的证书>
EOF

systemctl daemon-reload
systemctl restart docker

# 登录，用户名/密码 admin/Harbor12345
docker login harbor.dew.ms -u admin -p Harbor12345

# 测试
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 harbor.dew.ms/library/pause:3.1
docker push harbor.dew.ms/library/pause:3.1
----

==== nginx Ingress Controller

[source,bash]
----
# 使用如下方式将80 443暴露出来
helm install stable/nginx-ingress --name dew-nginx --namespace ingress-nginx \
    --set controller.kind=DaemonSet \
    --set controller.hostNetwork=true \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true
----

==== dashboard

[source,bash]
----
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque
EOF

# 安装，不使用代理方式需要加上 --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64
helm install stable/kubernetes-dashboard --name dew-dashboard --namespace kube-system \
    --set rbacAdminRole=true \
    --set ingress.enabled=true \
    --set-string ingress.annotations."nginx\.ingress\.kubernetes\.io/backend-protocol"="HTTPS" \
    --set ingress.hosts={dashboard.dew.ms} \
    --set ingress.tls[0].hosts={dashboard.dew.ms},ingress.tls[0].secretName=kubernetes-dashboard-certs

# 获取Token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep dew-dashboard-kubernetes-dashboard | awk '{print $1}')

# 使用Firefox访问
----

==== elasticsearch

TIP: https://github.com/elastic/helm-charts/blob/master/elasticsearch 注意仔细查看各参数设值的说明。

[source,bash]
----
# 创建PV
app=dew-elasticsearch-client
size=200Gi
# 请根据replicas的个数来决定下面PV的创建个数
for i in {0..1}; do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${app}
  name: ${app}-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/elasticsearch/${app}-${i}
    server: nfs.dew.ms
EOF
done

# 注意在NFS服务器上创建对应文件夹
for i in {0..1}; do
mkdir -p /data/nfs/elasticsearch/${app}-${i}
done

# TIP：如果pod没有启动成功，报错和路径权限问题有关，可尝试给PV的存储路径添加权限,如：
chmod 775 /data/nfs/elasticsearch/dew-elasticsearch-client-0
chmod 775 /data/nfs/elasticsearch/dew-elasticsearch-client-1


# 使用helm安装
helm repo add elastic https://helm.elastic.co

helm install --name dew-elasticsearch elastic/elasticsearch --namespace devops \
    --set imageTag=6.6.1 \
    --set clusterName=dew-elasticsearch \
    --set nodeGroup=client \
    --set masterService=dew-elasticsearch-client \
    --set replicas=2 \
    --set minimumMasterNodes=2 \
    --set volumeClaimTemplate.storageClassName="" \
    --set volumeClaimTemplate.resources.requests.storage=200Gi \
    --set fsGroup=0 \
    --set clusterHealthCheckParams="" \
    --set ingress.enabled=true \
    --set ingress.hosts={es.dew.ms}

    # pod调度相关配置,请根据需要进行设值
    --set nodeSelector."tag"="devops" \
    --set tolerations[0].key="key" \
    --set tolerations[0].operator="Equal" \
    --set tolerations[0].value="value" \
    --set tolerations[0].effect="NoSchedule" \
    --set nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="tag" \
    --set nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=devops \
    --set antiAffinity="hard" \  #该值可为soft
    --set antiAffinityTopologyKey="kubernetes.io/hostname" \

    # 若使用xpack security,请加上以下参数
    --set-string extraEnvs[0]."name"="xpack\.security\.enabled" \
    --set-string extraEnvs[0]."value"="true" \
    --set-string extraEnvs[1]."name"="xpack\.security\.authc\.accept_default_password" \
    --set-string extraEnvs[1]."value"="true" \
    --set-string extraEnvs[2]."name"="ELASTIC_USERNAME" \
    --set-string extraEnvs[2]."value"="elastic" \
    --set-string extraEnvs[3]."name"="ELASTIC_PASSWORD" \
    --set-string extraEnvs[3]."value"="123456"

    xpack安装相关文档说明：https://github.com/elastic/helm-charts/blob/master/elasticsearch/README.md#security

    * 开启xpack security的简单例子：
      . 进入容器内部
        kubectl exec -it dew-elasticsearch-client-0 -n devops /bin/sh
      . 激活30天试用license
        curl -H "Content-Type:application/json" -XPOST  http://localhost:9200/_xpack/license/start_trial?acknowledge=true
      . 修改密码：
        bin/elasticsearch-setup-passwords interactive
      . 测试：
        curl -u elastic -XGET 'localhost:9200/_cat/health?v&pretty'

----

TIP: 其他elasticsearch的helm chart : https://github.com/helm/charts/tree/master/stable/elasticsearch

==== fluentd

TIP: https://github.com/kiwigrid/helm-charts/tree/master/charts/fluentd-elasticsearch +
     https://kiwigrid.github.io/

[source,bash]
----
helm repo add kiwigrid https://kiwigrid.github.io

helm install kiwigrid/fluentd-elasticsearch --name dew-fluentd-es --namespace devops \
    --set elasticsearch.host=dew-elasticsearch-client \
    --set elasticsearch.logstash_prefix=logstash \
    # 若 ES 启用 xpack 的 security，加上以下参数
    --set elasticsearch.user=elastic \
    --set elasticsearch.password=123456
    # Prometheus 相关设置(需先安装prometheus-operator)
    --set service.type=ClusterIP \
    --set service.ports[0].name="monitor-agent" \
    --set service.ports[0].port=24231 \
    --set prometheusRule.enabled=true \
    --set prometheusRule.prometheusNamespace=devops \
    --set prometheusRule.labels.app=prometheus-operator \
    --set prometheusRule.labels.release=dew-prometheus-operator \
    --set serviceMonitor.enabled=true \
    --set serviceMonitor.labels.release=dew-prometheus-operator
    # 不使用代理要加上
    --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/fluentd-elasticsearch \
    --set image.tag=v2.4.0
    # pod调度相关配置,请根据实际需要进行设值
    --set nodeSelector."tag"="devops" \
    --set tolerations[0].key="key" \
    --set tolerations[0].operator="Equal" \
    --set tolerations[0].value="value" \
    --set tolerations[0].effect="NoSchedule"
----


==== kibana

TIP: https://github.com/helm/charts/tree/master/stable/kibana

[source,bash]
----

使用PVC
app=("kibana")
size=10Gi

for i in ${app[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: ${size}
  selector:
    matchLabels:
      app: ${i}
---
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/${i}
    server: nfs.dew.ms
EOF
done
# 注意在NFS服务器上加上文件路径,并创建需要的目录

helm install --name dew-kibana stable/kibana --namespace devops \
    --set image.tag="6.6.1" \
    --set env."ELASTICSEARCH_URL"="http://dew-elasticsearch-client:9200" \
    --set service.internalPort=5601 \
    --set ingress.enabled=true,ingress.hosts={kibana.dew.ms} \
    --set-string ingress.annotations."kubernetes\.io/ingress\.class"=nginx \
    --set-string ingress.annotations."kubernetes\.io/tls-acme"="true" \
    --set ingress.tls[0].hosts={kibana.dew.ms},ingress.tls[0].secretName=kibana-certs \
    --set dashboardImport.enabled=true \
    --set dashboardImport.dashboards."k8s"="https://raw.githubusercontent.com/monotek/kibana-dashboards/master/k8s-fluentd-elasticsearch.json" \
    --set serviceAccount.create=true,serviceAccountName=kibana \
    --set plugins.enabled=true \
    --set persistentVolumeClaim.enabled=true \
    --set persistentVolumeClaim.existingClaim=true \
    --set securityContext.enabled=true \
    --set securityContext.allowPrivilegeEscalation=true \
    --set securityContext.runAsUser=0 \
    --set securityContext.fsGroup=0

    # pod调度相关配置，请根据实际情况设值
    --set nodeSelector."tag"="devops" \
    --set tolerations[0].key="key" \
    --set tolerations[0].operator="Equal" \
    --set tolerations[0].value="value" \
    --set tolerations[0].effect="NoSchedule" \
    --set affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \

    # xpack security相关参数：
    --set image.repository=docker.elastic.co/kibana/kibana \
    --set env."XPACK_SECURITY_ENABLED"="true" \
    --set env."ELASTICSEARCH_USERNAME"="kibana" \
    --set env."ELASTICSEARCH_PASSWORD"="dew123456" \
    --set dashboardImport.xpackauth.enabled=true \
    --set dashboardImport.xpackauth.username=kibana\
    --set dashboardImport.xpackauth.password=dew123456

----

==== jaeger

TIP: https://github.com/jaegertracing/jaeger-operator

[source,bash]
----
kubectl create -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/crds/jaegertracing_v1_jaeger_crd.yaml
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/service_account.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/role_binding.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -
curl https://raw.githubusercontent.com/jaegertracing/jaeger-operator/master/deploy/operator.yaml \
    | sed "s/namespace: observability/namespace: devops/g" \
    | kubectl create -f -

# 使用elasticsearch作为jaeger的数据源
    # 若ES启用Xpack Security，则需要创建secret
    ELASTICSEARCH_USERNAME=elastic
    ELASTICSEARCH_PASSWORD=123456
    cat <<EOF | kubectl -n devops apply -f -
    apiVersion: v1
    kind: Secret
    metadata:
      name: jaeger-es-secrets
    type: Opaque
    data:
      ES_USERNAME: `echo -n $ELASTICSEARCH_USERNAME | base64`
      ES_PASSWORD: `echo -n $ELASTICSEARCH_PASSWORD | base64`
    EOF

    # 创建Jaeger实例
    cat <<EOF | kubectl apply -n devops -f -
    apiVersion: jaegertracing.io/v1
    kind: Jaeger
    metadata:
      name: jaeger
    spec:
      strategy: production
      storage:
        type: elasticsearch
        options:
          es:
            server-urls: http://dew-elasticsearch-client:9200
        secretName: jaeger-es-secrets  # 若ES启用Xpack Security，需要设置此项及创建secret
    EOF

    TIP: Jaeger实例可在不同namespace下创建使用，使用中请注意namespace的问题。
    使用sidecar的方式部署项目：https://github.com/jaegertracing/jaeger-operator#auto-injection-of-jaeger-agent-sidecars
    使用daemonset的方式部署项目：https://github.com/jaegertracing/jaeger-operator#agent-as-daemonset

# 修改Ingress
cat <<EOF | kubectl -n devops apply -f -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
  name: jaeger-query
spec:
  rules:
    - host: jaeger.dew.ms
      http:
        paths:
          - backend:
              serviceName: jaeger-query
              servicePort: 16686
            path: /
EOF

----

.pod的调度
目前jaeger-operator暂不支持直接设置，请关注该项目的更新情况。
可以自行给需要调度的pod的deployment添加限制条件。可参考： <<podAssignment>>

.Jaeger demo
[source,bash]
----
cat <<EOF | kubectl apply -f -
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    inject-jaeger-agent: "true"
    sidecar.jaegertracing.io/inject: "true"
  name: jaeger-demo
spec:
  template:
    metadata:
      labels:
        app: jaeger-demo
        version: v1
    spec:
      containers:
      - name: jaeger-demo
        image: jaegertracing/example-hotrod:1.10
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    inject-jaeger-agent: "true"
    sidecar.jaegertracing.io/inject: "true"
  name: jaeger-demo
  labels:
    app: jaeger-demo
spec:
  ports:
   - name: jaeger-demo
     port: 8080
     targetPort: 8080
  selector:
   app: jaeger-demo
EOF
----


==== prometheus-operator 和 grafana
TIP: https://github.com/helm/charts/tree/master/stable/prometheus-operator

  prometheus-operator结构：
    |--- prometheus-operator
    |--- prometheus
    |--- alertmanager
    |--- node-exporter
    |--- kube-state-metrics
    |--- service monitors to scrape internal kubernetes components
    |     |---kube-apiserver
    |     |---kube-scheduler
    |     |---kube-controller-manager
    |     |---etcd
    |     |---kube-dns/coredns
    |
    |--- grafana

===== prometheus-operator 使用PV

[source,bash]
----
# 创建PV,注意label的对应

app=prometheus-operator
components=("alertmanager" "prometheus")
size=100Gi

for i in ${components[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    component: ${i}
  name: dew-${app}-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/${app}/${i}
    server: nfs.dew.ms
EOF
done

# 在NFS服务器上创建相同路径
for i in ${components[@]};do
mkdir -p /data/nfs/${app}/${i}
done

----

===== 创建grafana的PVC和PV
[source,bash]
----
app=("grafana")
size=50Gi

for i in ${app[@]};do
cat <<EOF | kubectl -n devops apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: ${size}
  selector:
    matchLabels:
      app: ${i}
---
apiVersion: v1
kind: PersistentVolume
metadata:
  labels:
    app: ${i}
  name: dew-${i}
spec:
  capacity:
    storage: ${size}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /data/nfs/prometheus-operator/${i}
    server: nfs.dew.ms
EOF
done

# 注意在NFS服务器上创建相同路径
mkdir -p /data/nfs/prometheus-operator/grafana
----


===== 使用helm 安装

注意安装前先更新chart仓库 +
`helm repo update`

[source,yaml]
----
# 若需要对etcd进行监控，则需要先创建secret
kubectl -n devops create secret generic dew-prometheus-operator-etcd  --from-file=/etc/kubernetes/pki/etcd/ca.crt  --from-file=/etc/kubernetes/pki/etcd/peer.crt  --from-file=/etc/kubernetes/pki/etcd/peer.key

helm install stable/prometheus-operator --name dew-prometheus-operator --namespace devops \
    --set kubelet.serviceMonitor.https=true \
    --set prometheus.ingress.enabled=true \
    --set prometheus.ingress.hosts={prometheus.dew.ms} \
    --set alertmanager.ingress.enabled=true \
    --set alertmanager.ingress.hosts={prometheus.alertmanager.ms} \
    --set prometheusOperator.securityContext.runAsNonRoot=false \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
    --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.selector.matchLabels."component"="alertmanager" \
    --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.selector.matchLabels."component"="prometheus"
    # 对etcd监测相关参数
    --set prometheus.prometheusSpec.secrets[0]=dew-prometheus-operator-etcd \
    --set kubeEtcd.serviceMonitor.scheme=https \
    --set kubeEtcd.serviceMonitor.insecureSkipVerify=true \
    --set kubeEtcd.serviceMonitor.caFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/ca.crt" \
    --set kubeEtcd.serviceMonitor.certFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/peer.crt" \
    --set kubeEtcd.serviceMonitor.keyFile="/etc/prometheus/secrets/dew-prometheus-operator-etcd/peer.key"
    # 直接使用prometheus-operator的grafana，添加以下设置
    --set grafana.enabled=true \
    --set grafana.adminPassword=Dew123456 \
    --set grafana.defaultDashboardsEnabled=true \
    --set grafana.ingress.enabled=true \
    --set grafana.ingress.hosts={grafana.dew.ms} \
    --set grafana.ingress.tls[0].host={grafana.dew.ms},ingress.tls[0].secretName=dew-grafana \
    --set grafana.sidecar.dashboards.enabled=true \
    --set grafana.sidecar.dashboards.searchNamespace="devops"\
    --set grafana.sidecar.dashboards.label=grafana_dashboard \
    --set grafana.sidecar.datasources.enabled=true \
    --set grafana.sidecar.datasources.searchNamespace="devops" \
    --set grafana.sidecar.datasources.label=grafana_datasource \
    --set grafana.'grafana\.ini'.smtp.enabled="true" \
    --set grafana.'grafana\.ini'.smtp.host="smtp.163.com:25" \
    --set grafana.'grafana\.ini'.smtp.user=XXXXX@163.com \
    --set grafana.'grafana\.ini'.smtp.password=XXXXX \
    --set grafana.'grafana\.ini'.smtp.from_address="XXXXX@163.com" \
    --set grafana.'grafana\.ini'.smtp.skip_verify=true \
    --set grafana.persistence.enabled=true \
    --set grafana.persistence.existingClaim=dew-grafana
    # 可选设置
    --set grafana.'grafana\.ini'.server.root_url="https://grafana.dew.ms"

    # 如不使用代理,更换以下镜像仓库
    --set kube-state-metrics.image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-state-metrics

TIP: grafana默认用户名：admin,
查看密码：
kubectl get secret --namespace devops dew-prometheus-operator-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
grafana重置密码：进入grafana的容器内部后执行
grafana-cli admin reset-admin-password passwordvalue

INFO: 若有pod启动失败,报文件权限拒绝相关问题，很可能和PV的文件目录的权限有关，检查下权限是否一致，设置对应的securityContext进行排查。
例：
kubectl edit statefulset prometheus-dew-prometheus-operator-prometheus -n devops
设置securityContext为以下内容
      securityContext:
        fsGroup: 0
        runAsNonRoot: false
        runAsUser: 0

INFO: 若通过UI查看prometheus的target中，kube-scheduler、kube-controller处于down状态，是因为它们只能在宿主机上通过127.0.0.1访问，可使用以下操作：
    . 如果使用kubeadm启动的集群，初始化时的config.yml里可以加入如下参数
        controllerManagerExtraArgs:
          address: 0.0.0.0
        schedulerExtraArgs:
          address: 0.0.0.0
    . 已经启动后的使用下面命令更改就会滚动更新
        sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yaml
        sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml
      或者全部替换：
        sed -ri '/--address/s#=.+#=0.0.0.0#' /etc/kubernetes/manifests/kube-*
    . 参考文章：
      http://www.servicemesher.com/blog/prometheus-operator-manual/
      https://github.com/coreos/prometheus-operator/blob/master/Documentation/troubleshooting.md

# 监控APP
  1.首先需要将项目instrument
    参考文章：https://prometheus.io/docs/instrumenting/clientlibs/
  2.部署项目及创建进行监控的ServiceMonitor。
    注意ServiceMonitor的labels要含有Prometheus-operator创建的Prometheus的serviceMonitorSelector的label。
    详细文章：https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md#related-resources
----

[[prometheus-pod-assignment]]
===== pod的调度
pod调度helm安装相关参数，以下配置仅供参考
[source,yaml]
----
    --set alertmanager.alertmanagerSpec.nodeSelector."tag"="devops" \
    --set alertmanager.alertmanagerSpec.tolerations[0].key="key" \
    --set alertmanager.alertmanagerSpec.tolerations[0].operator="Equal" \
    --set alertmanager.alertmanagerSpec.tolerations[0].value="value" \
    --set alertmanager.alertmanagerSpec.tolerations[0].effect="NoSchedule" \
    # podAntiAffinity的值可以 hard 或 soft
    --set alertmanager.alertmanagerSpec.podAntiAffinity="hard" \
    --set alertmanager.alertmanagerSpec.podAntiAffinityTopologyKey="kubernetes\.io/hostname" \

    --set prometheusOperator.nodeSelector."tag"="devops" \
    --set prometheusOperator.tolerations[0].key="key" \
    --set prometheusOperator.tolerations[0].operator="Equal" \
    --set prometheusOperator.tolerations[0].value="value" \
    --set prometheusOperator.tolerations[0].effect="NoSchedule" \
    --set prometheusOperator.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set prometheusOperator.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set prometheusOperator.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \

    --set prometheus.prometheusSpec.nodeSelector."tag"="devops" \
    --set prometheus.prometheusSpec.tolerations[0].key="key" \
    --set prometheus.prometheusSpec.tolerations[0].operator="Equal" \
    --set prometheus.prometheusSpec.tolerations[0].value="value" \
    --set prometheus.prometheusSpec.tolerations[0].effect="NoSchedule" \
    # podAntiAffinity的值可以 hard 或 soft
    --set prometheus.prometheusSpec.podAntiAffinity=hard \
    --set prometheus.prometheusSpec.podAntiAffinityTopologyKey="kubernetes\.io/hostname" \

    --set kube-state-metrics.nodeSelector."tag"="devops" \
    --set kube-state-metrics.tolerations[0].key="key" \
    --set kube-state-metrics.tolerations[0].operator="Equal" \
    --set kube-state-metrics.tolerations[0].value="value" \
    --set kube-state-metrics.tolerations[0].effect="NoSchedule"

    --set nodeExporter.nodeSelector."tag"="devops" \
    --set nodeExporter.tolerations[0].key="key" \
    --set nodeExporter.tolerations[0].operator="Equal" \
    --set nodeExporter.tolerations[0].value="value" \
    --set nodeExporter.tolerations[0].effect="NoSchedule" \
    --set nodeExporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set nodeExporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set nodeExporter.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \

    --set grafana.nodeSelector."tag"="devops" \
    --set grafana.tolerations[0].key="key" \
    --set grafana.tolerations[0].operator="Equal" \
    --set grafana.tolerations[0].value="value" \
    --set grafana.tolerations[0].effect="NoSchedule"
    --set grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key="key" \
    --set grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].operator=In \
    --set grafana.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].values[0]=target-host-name \
----

===== 卸载
[source,yaml]
----
helm del --purge dew-prometheus-operator

kubectl delete crd prometheuses.monitoring.coreos.com prometheusrules.monitoring.coreos.com servicemonitors.monitoring.coreos.com alertmanagers.monitoring.coreos.com

kubectl delete pvc -n devops prometheus-dew-prometheus-operator-prometheus-db-prometheus-dew-prometheus-operator-prometheus-0 alertmanager-dew-prometheus-operator-alertmanager-db-alertmanager-dew-prometheus-operator-alertmanager-0

最后注意删除自己创建的PV、PVC
----
